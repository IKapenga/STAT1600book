%!Rnw root = ../../Master.Rnw

\chapter{Linear Regression}
\label{chap:ch14}

\section{Objective}

After completing this part, students should be able to:

\fbox{\parbox{14cm}{
\begin{itemize}

\item Interpret  a scatter plot.
\item Interpret the correlation coefficient $r$ \index{coefficient coefficient}  and the coefficient of determination $r^2$. \index{coefficient of determination}
\item Find and elucidate the regression line and use it to predict values of the response variable.
\item Put into words the concepts of the total, unexplained, and explained variation.  \index{unexplain variation} \index{explained variation}  \index{total variation}
\item Use correlation and regression techniques to evaluate two-variable relationships.

\end{itemize}

}}

\section{Simple Linear Regression}  \index{Linear Regression}

The following table contains data on winning bid price for 12 Saturn cars on eBay in July 2002.  The car mileage is also given, and the cars have been arranged in increasing order of Miles.

Here is a scatterplot of the data.  Since Price depends on Miles (not the other way around), we let Price be the $Y$-variable, or the \textit{response variable}.    Miles is the $X$-variable, or the \textit{explanatory variable}.

\begin{minipage}[ht]{7cm}
%\begin{table}[ht]
\centering 
\begin{tabular}{@{} c rr @{}} \hline 
Car & Miles & Price (\$) \\ \hline
1 & 9300 & 7100 \\
2 & 10565 & 15500 \\
3 & 15000 & 4400 \\
4 & 15000 & 4400 \\
5 & 17764 & 5900 \\
6 & 57000 & 4600 \\
7 & 65940 & 8800 \\
8 & 73676 & 2000 \\
9 & 77006 & 2750 \\
10 & 93739 & 2550 \\
11 & 146088 & 960 \\
12 & 153260 & 1025 \\ \hline
\end{tabular}
%\end{table}
\end{minipage}
\begin{minipage}[ht]{7cm}
% \begin{figure}[ht]
\centering
%\caption{
Scatterplot of Price (\$) vs. Miles

<<label=LBL14a, results="asis", echo=FALSE, out.width="7cm">>=
ml1 <- c(9300,10565,15000,15000,17764,57000,65940,73676,77006,93739,146088,153260)
prc1<- c(7100,15500,4400,4400,5900,4600,8800,2000,2750,2550,960,1025)
  plot(ml1, prc1, xlab="Miles", ylab="Price ($)")
@
% \end{figure}
\end{minipage}

\subsubsection{Problem:}

Based on the data, how much do I expect to get for a Saturn car that has been driven 60,000 miles?

Simple linear regression is a data analysis technique that tries to find a \textit{linear} pattern in the data.  We then use this line for prediction.

Notice that the points seem to fall around a \textit{straight line} sloping downwards.  Can you draw this line?  We will discuss one way to do this, called the \textit{least squares} (LS) method.  For now, suppose that the LS line has already been computed (we will do this later).  The LS line is overlayed on the scatterplot looks like Figure 14.1.

\begin{figure}[ht]
\centering
\caption{Least Squares (LS) Regression Line is overlayed on Scatterplot}
<<label=LBL14b, results="asis", echo=FALSE>>=
  tmp1 <- lm(prc1 ~ ml1)
  plot(ml1, prc1, xlab="Miles", ylab="Price ($)")
  abline(tmp1)
  aa0 <- sprintf("%.0f", tmp1$coefficients[1])
  aa1 <- sprintf("%.5f", tmp1$coefficients[2])
@  
\end{figure}

The formula for this line, in the form $Y= a + bX$,  is 

\begin{equation*}
  \texttt{Predicted Price} = \Sexpr{aa0} + (\Sexpr{aa1})(\texttt{Miles})
\end{equation*}

The \textit{slope} of the line is \Sexpr{aa1}, which means that predicted Price tends to drop 5 cents for every additional mile driven, or about \$512.70 for every 10,000 miles. The \textit{Y-intercept} of the line is \$ \Sexpr{aa0}; this should not be interpreted as the predicted price of a car with 0 mileage because the range of the data does not include cars with 0 miles.  

We can now use the line to predict the selling price of a car with 60000 miles.  What is the height or Y value of the line at $X = 60000$?  The answer is

\begin{equation*}
  \texttt{Predicted Price} = \Sexpr{aa0} + (\Sexpr{aa1})(60000) = \$5059.80
\end{equation*}

or about \$5000 or \$5100 or so.

\section{Calculating the Least Squares Regression Line}

One way to calculate the regression line is to use these five statistics:

$$ \bar{X}, S_{\bar{X}}, \bar{Y}, S_{\bar{Y}}, \texttt{ and } r  $$

(i.e., the mean and SD of $X$, the mean and SD of $Y$, and the correlation between $X$ and $Y$.)

\begin{center}
\fbox{\parbox{14cm}{
The least square regression line is given by the equation

\begin{equation*}
  \texttt{Predicted} = a + b X
\end{equation*}

where the slope $b$ and the intercept $a$ are calculated as 

\begin{eqnarray}
  b &=& r \frac{S_y}{S_x} \\
  a &=& \bar{Y} - b \bar{X}  \nonumber \\
\end{eqnarray}
}}
\end{center}

Next, we'll perform the calculations for the Saturn Price data. 

\begin{table}[ht]
\centering 
\begin{tabular}{@{} c rr @{}} \hline 
Car & Miles & Price (\$) \\ \hline
1 & 9300 & 7100 \\
2 & 10565 & 15500 \\
3 & 15000 & 4400 \\
4 & 15000 & 4400 \\
5 & 17764 & 5900 \\
6 & 57000 & 4600 \\
7 & 65940 & 8800 \\
8 & 73676 & 2000 \\
9 & 77006 & 2750 \\
10 & 93739 & 2550 \\
11 & 146088 & 960 \\
12 & 153260 & 1025 \\ \hline
Average & 61195 & 4999 \\
SD  & 50989 & 4079 \\
$r$ & \multicolumn{2}{c}{-0.641} \\ \hline
\end{tabular}
\end{table}

Using the formulas for slope and intercept in equation (14.1 and 14.2)

\begin{eqnarray*}
b &=& r \frac{S_y}{S_x} = -0.641 \frac{4079}{50989} = -0.05127 \\
a &=& \bar{Y} - b \bar{X} = 4999 - (-0.05127)(61195) = 8136  \\
\end{eqnarray*}

so that the regression line is

\begin{equation*}
PREDICTED = a + b X = 8136 + (-0.05127) X
\end{equation*}

In terms of the original variable names, the regression line is

\begin{equation*}
  \texttt{Predicted Price} = \Sexpr{aa0} + (\Sexpr{aa1}) \texttt{Miles} 
\end{equation*}

\section{More on Simple Regression}

Why is this called the least squares line?  The answer is best shown by example.

\begin{table}[ht]

<<label=LBL14c, results="asis", echo=FALSE>>=
  rs1 <- numeric(12)
  yh1 <- numeric(12)
  yh1 <- sprintf("%.2f", tmp1$fitted.values)
  rs1 <- sprintf("%.2f", tmp1$residuals)
  rs1a <- sprintf("%.2f", sum(tmp1$residuals^2))
@  
\centering 
\begin{tabular}{@{} c rrrc @{}} \hline 
Car & Miles & Price (\$) & PRED & RES = Y - PRED    \\ \hline
1 & 9300 & 7100 & \Sexpr{yh1[1]} & \Sexpr{rs1[1]} \\
2 & 10565 & 15500 & \Sexpr{yh1[2]} & \Sexpr{rs1[2]} \\
3 & 15000 & 4400 & \Sexpr{yh1[3]} & \Sexpr{rs1[3]} \\
4 & 15000 & 4400 & \Sexpr{yh1[4]} & \Sexpr{rs1[4]} \\
5 & 17764 & 5900 & \Sexpr{yh1[5]} & \Sexpr{rs1[5]} \\
6 & 57000 & 4600 & \Sexpr{yh1[6]} & \Sexpr{rs1[6]} \\
7 & 65940 & 8800 & \Sexpr{yh1[7]} & \Sexpr{rs1[7]} \\
8 & 73676 & 2000 & \Sexpr{yh1[8]} & \Sexpr{rs1[8]} \\
9 & 77006 & 2750 & \Sexpr{yh1[9]} & \Sexpr{rs1[9]} \\
10 & 93739 & 2550 & \Sexpr{yh1[10]} & \Sexpr{rs1[10]} \\
11 & 146088 & 960 & \Sexpr{yh1[11]} & \Sexpr{rs1[11]} \\
12 & 153260 & 1025 & \Sexpr{yh1[12]} & \Sexpr{rs1[12]} \\ \hline
\end{tabular}
\end{table}

The first car has $Miles = 9300$.  What is its predicted price?  The predicted value is 

\begin{equation*}
  \texttt{Predicted Price} = \Sexpr{aa0} + (\Sexpr{aa1})(9300) = \Sexpr{yh1[1]}
\end{equation*}

This predicted value missed the actual selling price $Y = 7100$.  By how much? By

\begin{equation*}
  \texttt{Residual} = 7100 + \Sexpr{yh1[1]} = \Sexpr{rs1[1]}
\end{equation*}

The negative value means actual value is too low.  This difference is called the residual.  

Small residuals (ignoring the sign) are good because this means the prediction was close  (Car 1 above was predicted well, but Car 2 was not -- the selling price is almost double what was predicted).  Therefore, a prediction line is good if it gives residuals that are as small as possible.  

\begin{center}
\fbox{\parbox{9cm}{
The sum of squared residuals is 
$$ SSE = (\Sexpr{rs1[1]})^2 + \cdots + (\Sexpr{rs1[12]})^2 = \Sexpr{rs1a} $$
}}
\end{center}

and is a measure of `overall size' of the residuals.  In the Saturn Price data, \\
$SSE =  107,805,718$.

\begin{center}
\fbox{\parbox{8cm}{
The least square line given by (14.1) will have a smaller SSE than any other straight line.
}}
\end{center}

This means that if you use any other intercept and slope combination besides $(a,b) = (8136,.05127)$, the new set of predicted values and residuals will give an SSE that is larger than, or at best equal to 107,805,718. 

\section{A 95\% Confidence Interval for Slope}

Is there a linear relationship between X and Y?  It seems obvious that selling price (Y) responds to a car's mileage (X), but in science, relationships are often not too obvious and need confirmation by data.  For example, does an individual's systolic blood pressure (Y) tend to increase with their cholesterol level (X)?   Is there a relationship between one's total number of years of education (X) and income (Y)?  In this section, we will investigate the strength of linear relationships by looking at the slope estimate.  Since the slope represents how much Y responds to changes in the X-value, we will calculate a 95\% confidence interval for the slope, and examine whether it excludes 0.  If it does, then we can rule out the likelihood that the slope is 0.  Thus, we conclude that there is a significant linear relationship between $X$ and $Y$.

We start by stating the formula for standard error:

\fbox{\parbox{14cm}{
The slope estimate $b$ tends to miss the true value $\beta$ by an amount called the \textit{standard error} of the slope, denoted SE of $b$, and calculated as:

\begin{equation}
  SE_b = \sqrt{ \frac{(1 - r^2) S_y^2}{(n - 2) S_x^2}} 
\end{equation}  
}}

The interval estimate is the familiar $b \pm 1.96(SE)$.  It is formally calculated as follows.

\fbox{\parbox{14cm}{
A 95\% confidence interval estimate for the slope of the regression line is given by: 

The slope estimate $b$ tends to miss the true value $\beta$ by 
an amount called the \textit{standard error} of the slope, 
denoted SE of $b$, and calculated as:

\begin{equation}
  b \pm 1.96 \sqrt{ \frac{(1 - r^2) S_y^2}{(n - 2) S_x^2}} 
\end{equation}  
}}

If this interval excludes 0, then the likelihood of zero slope is ruled out, and we conclude that there is a significant linear relationship between $X$ and $Y$.

Returning to our Saturn car price example, recall that $b = -0.05127$.  The standard error of this estimate is

\begin{equation*}
  SE_b = \sqrt{ \frac{(1 - (-0.641)^2) (4079)^2}{(12 - 2) (50989)^2}} = 0.01942 
\end{equation*}  

The 95\% confidence interval is 

\begin{equation*}
  -.05127 \pm 1.96 (0.01942) 
\end{equation*}  
\begin{equation*}
  (-.09,  -.01) 
\end{equation*}

Since this interval excludes 0, we conclude a significant relationship between car mileage and selling price.

\section{Key Words}

\twocolumn

\section{exercises}

\begin{exercies}

\begin{exercise}  % 1

1.	Consider the following data:

\begin{tabular}{@{} cc @{}} \hline
X & Y \\ \hline
-2 & 0 \\


\end{tabular}

\end{exercise}
\begin{solution}

\end{solution}




\end{exercises}


\onecolumn

% The correlation is a common and useful statistics.  It is a single number from $-1$ to $+1$ that indicates the strength of the relationship between the two variables.  If the number is between $-0.3$ and $+0.3 $, then we will say that the relationship is weak or non-existent. Otherwise, we will say that the relationship is either moderate or strong.  By the way, the closer we are to a $-1$ or a $+1$ the stronger the relationship.   The following example shows us how to interpret the results.
% 
% \subsection{Example 1}
% 
% Let's hypothesize that height of a male  affects the male's self-esteem.  We probably don't need to be concerned about the direction of causality; it is  unlikely that self-esteem causes a person's height.  We select a random sample of twenty males because we know that male heights are different from female heights.  By not using females heights  will simplify  the analysis.
% 
% \begin{minipage}[ht]{5cm}
% 
% {\small{
%  \begin{tabular}{@{} ccc @{}} \hline % Column formatting, @{} suppresses leading/trailing space
%      %  & \multicolumn{3}{c}{Category } \\ \hline
%      % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
% Person & Height & Self.esteem \\ \hline
% 1 & 1.73 & 4.11 \\
% 2 & 1.81 & 4.62 \\
% 3 & 1.57 & 3.83 \\
% 4 & 1.91 & 4.44 \\
% 5	& 1.47 & 3.25 \\
% 6	& 1.52 & 3.16 \\
% 7	& 1.71 & 3.87 \\
% 8	& 1.73 & 4.18 \\
% 9	& 1.81 & 4.39 \\
% 10 & 1.75 & 3.70 \\
% 11 & 1.73 & 3.51 \\
% 12 & 1.71 & 3.22 \\
% 13 & 1.61 & 3.73 \\
% 14 & 1.57 & 3.34 \\
% 15 & 1.52 & 3.45 \\
% 16 & 1.61 & 4.06 \\
% 17 & 1.65 & 4.17 \\
% 18 & 1.71 & 3.89 \\
% 19 & 1.61 & 3.40 \\
% 20 & 1.55 &3.61 \\ \hline
% \end{tabular} }}
% 
% \end{minipage} \hfill
% \begin{minipage}[ht]{11cm}
% 
% \centering
% 
% Self Esteem by Height
% 
% <<f12_1, echo=FALSE, fig.pos="ht", fig.align="center", fig.width=3.5, fig.height=3.5, fig.path="chapters/Chapter_12/figure/">>=
%   dt12_1 <- read.csv("data/esteem.csv",header=TRUE)
%   plot( dt12_1$Self.esteem ~ dt12_1$Height.m, xlab="Height", ylab="Self Esteem")
%   lm_1 <- lm( dt12_1$Self.esteem ~ dt12_1$Height.m )
%   abline(lm_1)
% @
% 
% \end{minipage}
% 
% % \subsubsection{Scatter plot}
% 
% 
% As we review the scatter plot, we see that the direction of the relationship is positive, (i.e., as height increases, self-esteem increases).
% 
% Based on the above graphic, there appears to be a relationship between the two variables -- height and self-esteem.  But is this relationship real?  We will use the five-step model to confirm our initial assumption.
% 
% %\begin{itemize}
% %\item
% {\textbf{Step 1.}}  Making Assumptions and Meeting Test Requirements.
% 
% % Requires the booktabs if the memoir class is not being used
% \begin{table}[htbp]
%    \centering
%     \caption{Model Assumptions }
%    %\topcaption{Table captions are better up top} % requires the topcapt package
%    \begin{tabular}{@{} rl @{}} \hline % Column formatting, @{} suppresses leading/trailing space
%       % \multicolumn{2}{c}{Item} \\
%       % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
%       1& Random sampling  \\
%       2      & Levels of Measurement are interval-ratio \\
%       3     & Bivariate normal distribution \\
%       4    & Linear relationship \\  \index{linear relationship}
%       5   & The variance of $Y$ scores is uniform for all $X$ values. \\
%       6  & Normality of residuals \\ \hline
%    \end{tabular}
%    \label{tab:t12_1}
% \end{table}
% 
% %\item
% {\textbf{Step 2.}}  State the Hypotheses. \\
%  $H_0: \rho = 0$ (there is no relationship between height and self-esteem) \\
%  $H_a: \rho \neq 0$  ($\rho$ represents the population \textbf{correlation coefficient}.)
% 
% % \item
%  {\textbf{Step 3.}}  Selecting the Sampling Distribution and Establishing the Critical Region.
% 
% 
%  % Requires the booktabs if the memoir class is not being used
% \begin{table}[htbp]
%    \centering
%    \caption{Components of correlation }
%    %\topcaption{Table captions are better up top} % requires the topcapt package
%    \begin{tabular}{@{} rcl @{}} \hline % Column formatting, @{} suppresses leading/trailing space
%       % \multicolumn{2}{c}{Item} \\
%       % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
%       Sampling Distribution & = & t-distribution \\
%       Alpha $(\alpha)$ & = & 0.05 (two-tailed) \\
%       Degrees of Freedom  & = & $ n - 2 = 20 - 2 = 18 $  \\
%       $t_{critical}$ & = & 2.10 \\ \hline
%    \end{tabular}
%    \label{tab:t12_2}
% \end{table}
% 
% %\item
% {\textbf{Step 4.}}  Computing the Test Statistic.
% 
% The test statistic is computed by the following equation:
% 
% \begin{align*}
% t_{obtained} &= r \sqrt{ \frac{n - 2}{1 - r^2}} \\
%               &= 0.721 \sqrt{ \frac{20 - 2}{1 - 0.721^2}} \\
%               &= 4.4145 \\
% \end{align*}
% 
% From figure \ref{fig:f12_2} on page \pageref{fig:f12_2} (MINITAB printout), we find Pearson Correlation coefficient, $r = 0.721$.  In step 4, we see the test statistic ($t_{obtained}$) is equal to 4.41.
% 
%   \begin{figure}[ht]
%     \centering
%     \caption{MINITAB Correlation Analysis}
%     \includegraphics[width=10cm]{chapters/Chapter_12/ext_figure/zCorrA.png} % requires the graphicx package
%     \label{fig:f12_2}
%  \end{figure}
% 
% %\item
% {\textbf{Step 5:}} Deciding and Interpreting the Results of the Test
% 
% Since the $t_{obtained}$ (test statistic) is greater than the $t_{critical}$ value ($ 4.41 > 2.10 $), we reject the null hypothesis.  Thus, we can say that there is a strong, positive relationship  between male height and male self-esteem.
% %\end{itemize}
% 
% % \pagebreak
% \section{What is Linear Regression?}  \index{linear regression}
% 
% Regression analysis is a method that we use when the explanatory and response variables are both numeric, i.e., interval-ratio variables, for example, weights, heights, volumes, and temperature.  The easiest way of knowing if the regression is the appropriate method is to see a scatter plot of the data.  The researcher will choose the linear regression model if there is an apparent (linear) relationship.  Let's examine an example that will help explain the process.
% 
% Let's consider the relationship between the percent change of city population growth  from 2000 and 2008 and the number of annual car thefts.  As a researcher, we review the level of measurement and find that both variables are interval-ratio.  So we consider using the scatter plot.  It has two dimensions, the x-axis which is the explanatory (X) variable and the y-axis which is the response (Y) variable that we want to predict.
% 
% \begin{minipage}[ht]{5cm}
% 
% \centering
% {\small{
%    \begin{tabular}{@{} cc @{}} \hline % Column formatting, @{} suppresses leading/trailing space
%      %  & \multicolumn{3}{c}{Category } \\ \hline
%      % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
%      Growth & Car.theft \\ \hline
%      9.8 & 118 \\
%      5.7 & 204 \\
%      1.3 & 294 \\
%      2.7 & 163 \\
%      26.8 & 764 \\
%      3.4 & 95 \\
%      16.8 & 392 \\
%      11.2 & 581 \\
%      8.6 & 601 \\
%      2.8 & 144 \\ \hline
%    \end{tabular}
% }}
% 
%   \end{minipage} \hfill
% \begin{minipage}[ht]{10cm}
% 
% \centering
% 
% Figure: 12.3, Car Theft by Growth
% 
% <<label=f12_3, echo=FALSE, fig.pos="ht", fig.align="center", fig.width=3.6, fig.height=3.6, fig.path="chapters/Chapter_12/figure/">>=
%   dt12_2 <- read.csv("data/EX12A.csv",header=TRUE)
%   plot( dt12_2$Car.theft ~ dt12_2$Growth, xlab="Growth", ylab="Car Theft")
%   lm_2 <- lm( dt12_2$Car.theft ~ dt12_2$Growth )
%   abline( lm_2 )
%   b_1 <- lm_2$coefficient[2]
%   b_0 <- lm_2$coefficient[1]
% 
%   xx_11 <- 20
%   xy_11 <- 0
% 
%   xy_22 <- b_0 + b_1 * 20
%   lines(c(xx_11,xy_11), c(xy_22, xy_22), col="red")
%   lines(c(xx_11, xx_11), c(xy_11, xy_22), col="red")
% @
% 
% \end{minipage}
% 
% \subsubsection{Interpreting Scatter Plot}
% 
% The pattern of points on the graph seems to indicate that  city growth (percent change from 2000 to 2008)  increases so does the number of auto thefts increase.  To demonstrate this pattern, we can draw a straight line that approximates the best fit for the points (See Figure 12.3).
% 
% \subsubsection{Does a Relationship Exist?}
% 
% The definition of an association states that the relationship between two variables will vary in the same direction either positive or negative.  As the \emph{X}-variable increases, the \emph{Y}-variable will increase (decrease if the direction is negative).  If a linear relationship exists between these two variables, then the best-fitted line through the points will not be parallel with the x-axis.
% % In other words, the \emph{X-Y} pair represents a point on the scatter plot.
% In a subsequent section, we will use a statistical method to test this hypothesis.
% 
% \subsubsection{How Strong is the association?}
% 
% The density around the fitted regression line assesses the strength of this relationship.  If the relationship is perfect, then the points of \emph{X - Y} will lie  on the fitted regression line.  Otherwise, the relationship becomes weaker when the data points are away from the fitted regression line.  What we are talking about here is the correlation coefficient.
% 
% From Figure \ref{fig:f12_4} on page \pageref{fig:f12_4} (MINITAB printout), we find Pearson Correlation coefficient, $r = 0.773$ indicating that we have a high correlation between city growth and car theft.  Note the arrow is pointing to the correlation coefficient.
% 
%   \begin{figure}[ht]
%     \centering
%     \caption{MINITAB Correlation Analysis}
%     \includegraphics[width=11cm]{chapters/Chapter_12/ext_figure/zEX12corr.png} % requires the graphicx package
%     \label{fig:f12_4}
%  \end{figure}
% 
% \subsubsection{Predicting response variable using the scatter plot}
% 
% We can use the scatter plot for prediction.  To show this procedure, let's look at the relationship between city growth and annual car thefts illustrated in Figure 12.3.  Let's say that we want to predict the number of car thefts in a city that has grown by 20 percent from 2000 to 2008.  The predicted score on \emph{Y} (indicated by $\hat{Y}$) to discriminate between the predictions of \emph{Y} from the actual \emph{Y} scores.  First, we should locate 20 on the x-axis and then draw a straight line parallel with the y-axis and to the regression line.  From the regression line, draw another line that is parallel to the x-axis to the y-axis.  The predicted \emph{Y} ($\hat{Y}$) is found at the point where the line crosses the y-axis.  We predict that cities that have grown by 20 percent from 2000 to 2008, the number of car thefts would be around 600 per year.  Later, we will develop a model for making this prediction.
% 
% \subsubsection{The Regression Line}
% 
% Using the above method is crude because trying to fit a line through a cloud of points by using a free hand.  A mathematical method has been developed and implemented in many statistical programs, such as SPSS, MINITAB, SAS, R, etc., to solve this fundamental equation:
% 
% \begin{equation}
% Y = a + bX
% \end{equation}
% 
% \begin{align*}
% Y &= Dependent \hspace{1mm} Variable \\
% a &= Y \hspace{1mm} intercept \\  \index{intercept}
% b &= Slope \\  \index{slope}
% X &= Independent \hspace{1mm} Variable
% \end{align*}
% 
% \subsubsection{Determining the y-intercept (a)}
% 
% From the MINITAB output Figure \ref{fig:f12_5}, let's determine the y-intercept of this  problem.
% 
%   \begin{figure}[ht]
%     \centering
%     \caption{MINITAB Regression Analysis}
%     \includegraphics[width=11cm]{chapters/Chapter_12/ext_figure/zRegD.png} % requires the graphicx package
%     \label{fig:f12_5}
%  \end{figure}
% 
%  After reviewing Figure \ref{fig:f12_5}, we find column B and row (Constant) (a) is 139.954.  Our interpretation of the y-intercept is 139.954 when \emph{X} = 0.  Now keep this number in mind as we determine the slope because we will use these two numbers to predict the annual car theft based on the percent change that a city experienced from 2000 to 2008.
% 
% \subsubsection{Determine the slope (b) }
% 
% From the MINITAB output Figure \ref{fig:f12_5}, let's identify the slope of this  problem.
% 
% After reviewing Figure \ref{fig:f12_5}, we find column B and row Growth that the slope (b) is 22.665.  Our interpretation of the slope is that as the percentage change of the cities growth increases by 1 percent, the number of car thefts increases by 22.665 per year.  Let's evaluate the slope with the five step model.
% 
% \begin{itemize}
% \item {\textbf{Step 1.}}  Making Assumptions and Meeting Test Requirements.
% 
% % Requires the booktabs if the memoir class is not being used
% \begin{table}[htbp]
%    \centering
%     \caption{Model Assumptions}
%    %\topcaption{Table captions are better up top} % requires the topcapt package
%    \begin{tabular}{@{} rl @{}} \hline % Column formatting, @{} suppresses leading/trailing space
%       % \multicolumn{2}{c}{Item} \\
%       % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
%       1 & Random sampling  \\
%       2       & Levels of Measurement are interval-ratio \\
%       3       & Bivariate normal distribution \\
%       4       & Linear relationship \\
%       5       & Variance of $Y$ scores is uniform for all $X$ values. \\
%       6       & Normality of residuals. \\ \hline
%    \end{tabular}
%    \label{tab:t12_3}
% \end{table}
% 
% \item {\textbf{Step 2.}}  State the Hypotheses. \\
%  $H_0: \beta = 0$ (there is no relationship between percent change of a city and car theft) \\
%  $H_a: \beta \neq 0$ (where $\beta$ represents the population \textbf{slope}.)
%  \item {\textbf{Step 3.}}  Selecting the Sampling Distribution and Establishing the Critical Region.
% 
% 
%  % Requires the booktabs if the memoir class is not being used
% \begin{table}[htbp]
%    \centering
%     \caption{Components of correlation}
%    %\topcaption{Table captions are better up top} % requires the topcapt package
%    \begin{tabular}{@{} rcl @{}} \hline % Column formatting, @{} suppresses leading/trailing space
%       % \multicolumn{2}{c}{Item} \\
%       % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
%       Sampling Distribution & = & t-distribution \\
%       Alpha $(\alpha)$ & = & 0.05 (two-tailed) \\
%       Degrees of Freedom  & = & $ n - 2 = 10 - 2 = 8 $  \\
%       $t_{critical}$ & = & 2.3060 \\ \hline
%    \end{tabular}
%    \label{tab:t12_4}
% \end{table}
% 
% \item {\textbf{Step 4.}}  Determine the Test Statistic.
% 
% The test statistic is determined from  the following table.
% 
% From Figure \ref{fig:f12_6}, (MINITAB printout), we find the slope (b) = 22.665.  The test statistic $t$ is 3.443, from row Growth and column $t$.
% 
%   \begin{figure}[ht]
%     \centering
%     \caption{MINITAB Regression Analysis}
%     \includegraphics[width=11cm]{chapters/Chapter_12/ext_figure/zRegD.png} % requires the graphicx package
%     \label{fig:f12_6}
%  \end{figure}
% 
% \item {\textbf{Step 5:}} Making a Decision and Interpreting the Results of the Test
% 
% Since the $t_{obtained}$ (test statistic) is greater than the $t_{critical}$ value ($ 3.443 > 2.2060 $), we reject the null hypothesis. Thus we can interpret the slope by saying as the percent change in  city increases by one, the number of annual car thefts increases by 22.7.
% 
% \end{itemize}
% 
% \subsubsection{Prediction}
% 
% Now let's put it all together.  We have an equation.  We will use the same \emph{X}-value = 20 that we used when we used a graphical solution for this problem.
% 
% \begin{align*}
% \hat{Y} &= a + b(X) \\
% CarThefts &= 139.954 + 22.665(Growth) \\
%           &= 139.954 + 22.665(20) \\
%           &= 593.3
% \end{align*}
% 
% We can now say that a city that had percent change growth of 20 percent will have an estimated 593 automobiles stolen in one year.
% 
% \subsubsection{Interpreting the Regression}
% 
% What variables can she collect to predict the final exam score of an Introductory Statistics course?  Statistics instructors have been asking this question for a long time.  An instructor gathered,  (four explanatory variables (MT1, MT2, Proj, HW)), data from 64 of her statistics class students and did the following analysis.
% 
% \subsubsection{Descriptive Statistics}
% 
% Table \ref{tab:t12_5} reports the basic descriptive statistics for the variables.  The average final exam score is 156.1 with a score ranging  from 75 to 191.  The mean mid-term  test one score is 86.1 with a score ranging from 50 to 99.  As consultants, we should ask ourselves, what explanatory variables should we use to predict the final exam score.
% 
%  % Requires the booktabs if the memoir class is not being used
% \begin{table}[htbp]
%    \centering
%     \caption{Descriptive Statistics }
%    %\topcaption{Table captions are better up top} % requires the topcapt package
%    \begin{tabular}{@{} lccccc @{}} \hline % Column formatting, @{} suppresses leading/trailing space
%        &  \multicolumn{5}{c}{Descriptive Statistics} \\
%       % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
%       Statistics & Final & MT1 & MT2 & Proj & HW \\ \hline
%       Mean & 156.1 & 86.1 & 74.2 & 10.1 & 72.3 \\
%       Standard deviation & 24.47 & 11.51 & 18.59 & 1.16 & 11.69 \\
%       N & 64 & 64 & 64 & 64 & 64 \\ \hline
%    \end{tabular}
%    \label{tab:t12_5}
% \end{table}
% 
% \begin{itemize}
% \item Where
%   \begin{itemize}
%   \samepage
%   \item Final --  Exam score
%   \samepage
%   \item MT1 -- Midterm exam one score
%   \samepage
%   \item MT2 -- Midterm exam two score
%   \samepage
%   \item Proj -- Project score
%   \samepage
%   \item HW -- Homework score
%   \samepage
%   \end{itemize}
% \end{itemize}
% 
% 
% \subsubsection{Correlation matrix}
% 
%  % Requires the booktabs if the memoir class is not being used
% \begin{table}[htbp]
%    \centering
%     \caption{Pearson Correlation Coefficient}
%    \begin{tabular}{@{} lccccc @{}} \hline % Column formatting, @{} suppresses leading/trailing space
%       % &  \multicolumn{5}{c}{Pearson Correlation} \\
%       % \cmidrule(r){1-2} % Partial rule. (r) trims the line a little bit on the right; (l) & (lr) also possible
%       Statistics & Final & MT1 & MT2 & Project & HW \\ \hline
%       Final & 1  \\
%       MT1   & \fbox{0.751} & 1 \\
%       MT2   & 0.543 & 0.454 & 1 \\
%       Proj & 0.087 & 0.101 & -0.032 & 1 \\
%       HW      & 0.273 & 0.301 & 0.218 & 0.093 & 1 \\ \hline
%    \end{tabular}
%    \label{tab:t12_6}
% \end{table}
% 
% After reviewing the correlation matrix (Table \ref{tab:t12_6}), we see that at the intersection of Final and MT1 has a correlation coefficient ($r = 0.751$) which is the largest number in the matrix.  This number tells us that we have a strong positive relationship between the mean final exam score and the mean mid-term exam one score.  The coefficient of determination ($r^2 = 0.564$) which tells us that the mid-term one score explains 56.4 percent of the variance in the final exam score.
% 
% \subsubsection{Scatter plot}  \index{scatter plot}
% 
% The next step in assessing an association between two interval-ratio variables is the scatter plot.  Figure \ref{fig:f12_7} displays the final exam scores on the vertical, \emph{Y}, axis, and the mid-term evaluation one score on the horizontal, \emph{X}, axis.
% 
% <<label=f12_7, echo=FALSE, fig.pos="ht", fig.align="center", fig.width=8, fig.height=8, fig.cap="Mid-term 1 Exam", out.width="6cm", fig.path="chapters/Chapter_12/figure/">>=
%   dt12_3 <- read.csv("data/grades.csv",header=TRUE)
%   plot( dt12_3$Final ~ dt12_3$MT1, xlab="Mid-Term 1 ", ylab="Final")
%   lm_3 <- lm( dt12_3$Final ~ dt12_3$MT1 )
%   abline( lm_3 )
%   b_1 <- lm_3$coefficient[2]
%   b_0 <- lm_3$coefficient[1]
% @
% 
% What can we say about the relationship?  The regression line is \emph{not} horizontal, so there is an association between these two variables.  The graph shows points scattered around the regression line, and we can see that we have a strong positive relationship.
% 
% \subsubsection{Regression Equation}  \index{regression equation}
% 
% The next step is to determine the regression equation.
% 
%   \begin{figure}[ht]
%     \centering
%     \includegraphics[width=12cm]{chapters/Chapter_12/ext_figure/zRegE.png} % requires the graphicx package
%     \caption{MINITAB Regression Analysis }
%     \label{fig:f12_8}
%  \end{figure}
% 
% After reviewing Figure \ref{fig:f12_8}, we find the \emph{Y}-intercept (\emph{a}) is 18.586, row (constant) and column B.  The slope (\emph{b}) is 1.597, row MT1 and column B.
% 
% \begin{align*}
%     Y &= a + bX \\
% Final &= 18.586 + 1.597(MT1)
% \end{align*}
% 
% The interpretation of the slope is as mid-term one exam scores increase by one unit, the final exam score increase by 1.597 points.
% 
% \subsubsection{Diagnostics}
% 
% \begin{itemize}
% \item Coefficient of determination:  \index{coefficient determination}
%   \begin{itemize}
%   \item $r^2 = \frac{explained Variation}{total Variation}$
%   \item We measure the explained variation from the average of the \emph{Y}-variable to the predicted \emph{Y}-variable.
%   \item We compute the unexplained variation from the observed \emph{Y}-variable to the predicted \emph{Y}-variable.
%   \item We determine the total change from the observed \emph{Y}-variable to the average of the \emph{Y}-variable.
%   \end{itemize}
% 
% \item There is one other thing that we should look at --  residuals.
%   \begin{itemize}
%   \item For each data point, we should compute the difference:
% 
%   \begin{equation}
%   Residual_i =  observedValue_i - predictedValue_i
%   \end{equation}
% 
% 
%   \item The histogram of the residuals should be  bell-shaped as in Figure \ref{fig:f12_9}.
% 
% 
% %\begin{minipage}[ht]{6.5cm}
% 
%   <<label=f12_9,  echo=FALSE, fig.pos="ht", fig.align="center", fig.width=8, fig.height=8, fig.cap="Histogram of Residuals", out.width="6cm", fig.path="chapters/Chapter_12/figure/">>=
%   hist(lm_3$residuals)
%   @
% 
% %\end{minipage} \hfill
% %\begin{minipage}[ht]{6.5cm}
% 
%   <<label=f12_10, echo=FALSE, fig.pos="ht", fig.align="center", fig.width=8, fig.height=8, fig.cap="Residual Plot", out.width="6cm", fig.path="chapters/Chapter_12/figure/">>=
% 
%   plot( lm_3$residuals ~ lm_3$fitted.values, xlab="Final -- fitted values ", ylab="Residuals")
%   abline( 0,0 )
% @
% 
% %\end{minipage}
% 
%   \item Let's look at Figure \ref{fig:f12_9}.  Here we can draw a nearly normal-shaped curve over the histogram.
%   \end{itemize}
% 
% \item Another assumption that we need to consider is homoscedasticity, i.e.,  \index{homoscedasticity} is the variance of \emph{Y} scores uniform across all values of \emph{X}.
% 
%   \begin{itemize}
% 
%   \item Figure \ref{fig:f12_10} shows a pattern of points that is uniform.
% 
%   \end{itemize}
% 
% \end{itemize}
% 
% \subsubsection{Summary}
% 
% To conclude this analysis, the linear regression equation, the correlation coefficient, and coefficient of determination suggest that there exist a positive and strong relationship between the first mid-term exam score  and the final exam score.  The amount of unexplained variation (total variation - explained variation, i.e., 100 - 55 = 45) suggests that there may be other variables besides first mid-term exam that have a significant influence on the final exam score.
% 
% We should note one limitation of the simple linear regression.  First correlation is not the same as causation.  In other words, two variables that are highly correlated does not necessarily suggest that there is a causal relationship.  We could take these results to support the proposition that higher mid-term exam scores lead to higher final exam scores, the mere existence of a relationship -- even a strong connection in the direction predicted by the theory -- does not prove that one variable causes the other.
% 
% \section{Key Words}
% 
% \colorbox{lgray}{\parbox{15cm}{
% \begin{minipage}[ht]{6cm}
% \begin{itemize}
% \item Bivariate normal distribution
% \item Coefficient of discrimination
% \item Correlation matrix
% \item Explained variation
% \item Homoscedasticity
% \item Linear relationship
% \item Pearson's r
% \end{itemize}
% 
% \end{minipage} \hfill
% \begin{minipage}[ht]{6cm}
% \begin{itemize}
% \item Regression line
% \item Scatter plot
% \item Slope
% \item Total variation
% \item Unexplained variation
% \item \emph{Y}-intercept
% \item $\hat{Y}$
% \end{itemize}
% \end{minipage}
% }}
% 
% \twocolumn
% \section{Exercises}
% 
% \begin{exercises}
% 
%   \begin{exercise} % 1
% 
% <<label=f12_11, echo=FALSE, fig.pos="ht", fig.align="center", fig.width=3, fig.height=3, fig.cap="Beers vs. BAC", fig.path="chapters/Chapter_12/figure/">>=
%      dt12_4 <- read.csv("data/beers.csv",header=TRUE)
%      plot( dt12_4$BAC ~ dt12_4$NB, xlab="NB", ylab="BAC")
%      lm_4 <- lm( dt12_4$BAC ~ dt12_4$NB )
%      abline( lm_4 )
%      b_1 <- lm_4$coefficient[2]
%      b_0 <- lm_4$coefficient[1]
%      rho1 <- cor(dt12_4$BAC, dt12_4$NB)
%      rho2 <- rho1^2
% @
% 
% The next three exercises use this scenario.  How well does the number of beers a student drinks predict his or her Blood Alcohol Content (BAC)? Sociology researchers, at Ohio State University, wanted to know if there is a relationship between the amount of beer consumed and BAC. The researchers assigned the number of cans of beer to each student. After each student had consumed the assigned number of beers,  thirty minutes later, an officer of the law measured the students BAC. \citep{OSU2016}  One student drank nine beers.  You see from the scatter plot \ref{fig:f12_11} that his BAC was about
% 
%     \begin{enumerate}
%     \item 0.014
%     \item 0.14
%     \item 1.4
%     \item 14
%     % \item 140
%     \end{enumerate}
% 
%     \framebox[5cm][l]{ Answer: }
% 
%   \end{exercise}
%   \begin{solution}  % 1
% 
%      $ BAC = -0.012 + 0.017 (9) = 0.14 $
%   \end{solution}
% 
%   \begin{exercise} % 2
% 
% Use the information from \\ exercise 1.  The scatterplot \ref{fig:f12_11} on page \pageref{fig:f12_11} shows
% 
%       \begin{enumerate}
%       \item a weak negative relationship.
%       \item a moderately high negative correlation.
%       \item almost no connection.
%       \item a small positive correlation.
%       \item a moderately high positive straight-line relationship between some beers and BAC.
%       \end{enumerate}
% 
%     \framebox[7cm][l]{ Answer: }
%   \end{exercise}
%   % \begin{solution}  % 2
%   %
%   %   A moderately strong positive straight-line relationship between number of beers and BAC.
%   % \end{solution}
% 
%   \begin{exercise} % 3
% 
% Use the information from \\ exercise 1.  A plausible value of the correlation between number of beers and blood alcohol content, based on the scatterplot, is
% 
%       \begin{enumerate}
%       \item $r =  -0.875$
%       \item $r =  -0.765$
%       \item $r$ close to 0
%       \item $r = 0.765$
%       \item $r = 0.875$
%       \end{enumerate}
% 
%     \framebox[7cm][l]{ Answer: }
%     \end{exercise}
%     \begin{solution}  % 3
% 
% The correllation coefficient is $r = \Sexpr{rho1} = \sqrt{r^2 }$ where  $r^2$ is the coefficient of determination.
%     \end{solution}
% 
%   \begin{exercise} % 4
% 
% <<label=f12_14a, results="asis", echo=FALSE, fig.pos="ht", fig.align="center", fig.width=3, fig.height=3, fig.cap="Resp. Ht vs. Inc.", fig.path="chapters/Chapter_12/figure/">>=
% 
% dt12_5 <- read.csv("data/GSS2014.csv", header=TRUE)
%   ## dt12_5 <- read.csv("~/Desktop/GSS2014 copy.csv", header=TRUE)
% x12_1 <- cbind( dt12_5$HEIGHT, dt12_5$rincom06 )            ## select only the respondent's height and income
% x12_2 <- x12_1[ (dt12_5$HEIGHT > 0 & dt12_5$HEIGHT < 98) &
% 						  (dt12_5$rincom06 > 0 & dt12_5$rincom06 < 26),  ]          ## select only valid data
% nrw12 <- length( x12_2[ , 1] )
% x12_3 <- x12_2[ sample( 1:nrw12, size= 100, replace = TRUE),  ]
% Height <- x12_3[,1]
% Income <- x12_3[,2]
% plot(Height, Income, xlab="Height(inch)", ylab="Income(K dollars)", main="Plot of Ht vs. Inc." )
% 
% cor12 <- cor(Income, Height)
% cor12p <- sprintf("%.4f", cor12)
% rSq   <- cor12^2
% 
% m12 <- lm(Income ~ Height)
% abline(m12)
% 
% @
% 
% STEP 1: In the next six tasks use Figure \ref{fig:f12_14a}, we will use the data from \\ GSS2014 in this exercise.  We have been asked to examine the relationship between a person's height and a person's income.  Here we have {\textit{income}} which is an  interval-ratio type of variable while {\textit{height}} is also an interval-ratio type of variable.  These types of variables require that we use a Simple Linear Regression (SLR) analysis.  So step 1 in the process of analysis \emph{is to choose} the independent and dependent variables.  Note: In the GSS2014 dataset, {\textit{height}} is known as HEIGHT and {\textit{income}} is known as rincom06.  The correlation coefficient is \Sexpr{cor12p}.
% 
%     \framebox[7cm][l]{ Answer: }
%     \end{exercise}
%     % \begin{solution}  % 4
%     %    The independent variable is HEIGHT and the dependent variable is rincom06.
%     % \end{solution}
% 
%   \begin{exercise} % 5
% 
% STEP 2: Using the information from exercise four, we must state the hypotheses about its relationship between {\textit{income}} and {\textit{height}}.  Based on our experience, how should we state our hypotheses?
% 
%     \framebox[7cm][l]{ Answer: }
%     \end{exercise}
%     \begin{solution}  % 5
% 
%        $H_0: \beta = 0" vs. $H_1: \beta \ne 0$
%     \end{solution}
% 
%   \begin{exercise} % 6
% 
% STEP 3: Using the information from exercise four, determine the coefficient of determination. \index{coefficient of determination}  Recall that this value is also referred to as r-squared.
% 
%     \framebox[7cm][l]{ Answer: }
%     \end{exercise}
%     % \begin{solution}  % 6
%     %
%     %    The correlation coefficient between the respondent's height and income is \Sexpr{rSq}
%     % \end{solution}
% 
%   \begin{exercise} % 7
% 
% STEP 4: Using the information from exercise four, record the independent, dependent variables, and the correlation coefficient.  Record the requested information in table \ref{tab:t12_7} on page \pageref{tab:t12_7}.
% 
%   %  \framebox[8cm][l]{ Answer: }
%     \end{exercise}
%     \begin{solution}  % 7
% 
%     \begin{table}[ht]
%     \centering
%     \begin{tabular}{lr} \hline
%         &  \multicolumn{1}{c}{Independent} \\ \hline
%     Dep. Var. & 1. height \\ \hline
%     1. income  &   \Sexpr{cor12} \\ \hline
%     \end{tabular}
%     \end{table}
%     \end{solution}
% 
%       \begin{exercise} % 8
% 
% STEP 5: Using the information from exercise four, describe the results of the independent variable.  Identify variables that we tested, their strength and direction of the relationship.  We should distinguish the relationship in general terms and refer the statistical value in parentheses.  Also not whether the hypotheses were supported.
% 
%     \framebox[7cm][l]{ Answer: }
% 
%     \end{exercise}
% %     \begin{solution}  % 8
% %
% % For a sample of 40 subjects, we tested the relationship between height and income.  We found a weak positive relationship using the correlation coefficient (\Sexpr{cor12}).  We then looked at the slope:  as {\textit{height}} increases by one inch, {\textit{income}} increases by \Sexpr{m12$coefficients[2]} with a p-value of
% %     \end{solution}
% 
%   \begin{exercise} % 9
% 
% STEP 1:  In the next six tasks, we will use a sample of 100 subjects from the GSS2014 data in this exercise.  We have been asked to examine the relationship between a person's income, age, and ``not married'' and a person's happiness.  Here we have variables which are the interval-ratio type of variable.  These types of variables require that we use a Simple Linear Regression (SLR) analysis table \ref{tbl12_15a} on page \pageref{tbl12_15a}.  So step 1 in the process of analysis is choosing independent and dependent variables.  Note: In the GSS2014 dataset, {\textit{happy (1)}} is known as GENERAL \\ HAPPINESS, {\textit{income06(2)}} is known as TOTAL FAMILY INCOME, {\textit{age(3)}} is known as AGE OF RESPONDENT, and {\textit{absingle(4)}} is known as \\ NOT MARRIED.
% 
% <<f12_15, results="asis", echo=FALSE, fig.pos="ht", fig.align="center", fig.width=2.5, fig.height=2.5, fig.cap="Respondent's Height vs. Income", fig.path="chapters/Chapter_12/figure/">>=
% 
% x12_9 <- cbind( dt12_5$happy, dt12_5$age, dt12_5$income06, dt12_5$absingle )            ## select only the respondent's height and income
% x12_10 <- x12_9[ (dt12_5$happy > 0 & dt12_5$happy < 8) &
% 						  (dt12_5$income06 > 0 & dt12_5$income06 < 26) &
% 						  (dt12_5$age  > 0) & (dt12_5$age < 98) &
% 						  (dt12_5$absingle > 0) & (dt12_5$absingle < 8),  ]  ## select only valid data
% nrw4 <- length( x12_10[ , 1] )
% x12_11 <- x12_10[ sample( 1:nrw4, size= 100, replace = TRUE),  ]
% Happiness  <- x12_11[,1]
% Age        <- x12_11[,2]
% Income     <- x12_11[,3]
% NotMarried <- x12_11[,4]
% 
%   cor13 <- cor(x12_11)
% 
%   print(xtable(cor13, caption='Correlation Coefficients Table', label="tab:12_15b"), caption.placement="top")
% 
%   m13 <- lm( Happiness ~  Age + Income + NotMarried )
% @
% 
% What are the dependent and independent variables?
% 
% 
%     \framebox[7cm][l]{ Answer: }
%     \end{exercise}
%     \begin{solution}  % 9
% 
%        The independent variable is income, age, and absingle and the dependent variable is happiness.
%     \end{solution}
% 
%   \begin{exercise} % 10
% 
% STEP 2: Using the information from exercise nine, we must state the hypotheses about its relationship between {\textit{Happiness}}, {\textit{income}},  {\textit{age}}, {\textit{NotMarried}}.  Based on our experience, how should we state our hypotheses?
% 
%     \framebox[7cm][l]{ Answer: }
% 
%     \end{exercise}
%     % \begin{solution}  % 10
%     %
%     %    $H_0: \beta_1 = \beta_2 = \beta_3 = 0$ \\
%     %    $H_1:$ at least one slope is not equal zero. \\
%     %    where $\beta_1$ is the slope for Happiness and Income,  $\beta_2$ is the slope for Happiness and Age and $\beta_3$ is         the slope for Happiness and single.
%     % \end{solution}
% 
%   \begin{exercise} % 11
% 
% STEP 3: Using the information from exercise nine, determine the coefficient of determination.
% 
%     \framebox[7cm][l]{ Answer: }
%     \end{exercise}
%     \begin{solution}  % 11
% 
%        Read the results from the computer generated output.
%     \end{solution}
% 
%   \begin{exercise} % 12
% 
% STEP 4: Using the information from exercise nine, record the independent, dependent variables, and the correlation coefficients information onto table \ref{tab:t12_12} on page \pageref{tab:t12_12}.
% 
%   %  \framebox[7cm][l]{ Answer: }
%     \end{exercise}
%     % \begin{solution}  % 12
%     %
%     %   The results are in the table.
%     % \end{solution}
% 
%   \begin{exercise} % 13
% 
% STEP 5:  Using the information from exercise nine, record the independent, dependent variables, and the correlation coefficients.
% 
%   %  \framebox[8cm][l]{ Answer: }
%     \end{exercise}
%     \begin{solution}  % 13
% 
%   The results are in the table.
%     \end{solution}
% 
%   \begin{exercise} % 14
% 
% STEP 1: A mail-order catalog business selling supplies for computers manages a centralized warehouse for disemination of products that customers ordered.  The  leadership team is examining the process of distribution from the warehouse and is considering the factors that affect the overall cost of operation.  It now has a small handling fee for each order, regardless the amount of the order.  Over the past 24 months, the team collected data such as the \emph{distribution cost(1)} (\$000), \emph{Sales(2)} (\$000), and the \emph{number of orders received(3)}.
% 
% % STEP 1:  In the next six tasks, we will use all of the data from GSS2014 in this exercise.  We have been asked to examine the relationship between a person's income, age, and ``not married'' and a person's happiness.  Here we have variables which are  interval-ratio type of variable.  These types of variables require that we use a Simple Linear Regression (SLR) analysis.  So step 1 in the process of analysis is choosing independent and dependent variables.  Note: In the GSS2014 dataset, {\textit{happy}} is known as GENERAL HAPPINESS, {\textit{income06}} is known as TOTAL FAMILY INCOME, {\textit{age}} is known as AGE OF RESPONDENT, and {\textit{absingle}} is known as NOT MARRIED.
% 
% <<f12_16, results="asis", echo=FALSE, fig.pos="ht", fig.align="center", fig.width=3.5, fig.height=3.5, fig.cap="Respondent's Height vs. Income", fig.path="chapters/Chapter_12/figure/">>=
% 
%   ## dt12_5 <- read.csv("~/github/STAT1600book/chapters/Chapter_12/data/warehouseCost.csv", header=TRUE)
%   dt12_5 <- read.csv("data/warehouseCost.csv", header=TRUE)
%   x12_10 <- cbind(dt12_5$DistCost, dt12_5$Sales, dt12_5$Orders)
% 
%   nrw4 <- length( x12_10[ , 1] )
% ## x12_11 <- x12_10[ sample( 1:nrw4, size= 100, replace = TRUE),  ]
%   DistCost <- x12_10[,1]
%   Sales    <- x12_10[,2]
%   Orders   <- x12_10[,3]
% 
%   cor14 <- cor(x12_10 )
% 
%   print(xtable(cor14, caption="Correlation Coefficient", label="tab: t12_16a"), caption.placement="top")
% 
%   m14 <- lm( DistCost ~ Sales + Orders )
%   aov16 <- aov(m14)
% @
% 
% What are the dependent and independent variables?
% 
%     \framebox[7cm][l]{ Answer: }
% 
%     \end{exercise}
%     % \begin{solution}  % 14
%     %
%     %    The independent variables are  Sales and  Orders and the dependent variable is DistCost.
%     % \end{solution}
% 
% 
%   \begin{exercise} % 15
% 
% STEP 2: Using the information from exercise 14, we must state the hypotheses about its relationship between {\textit{DistCost}}, \\ {\textit{Sales}}, and {\textit{Orders}}.  Based on our experience, how should we state our hypotheses?
% 
%     \framebox[7cm][l]{ Answer: }
%     \end{exercise}
%     \begin{solution}  % 15
% 
%        $H_0: \beta_1 = \beta_2 =  = 0$ \\
%        $H_1:$ at least one slope is not equal zero. \\
%        where $\beta_1$ is the slope for DistCost and Sales,  $\beta_2$ is the slope for DistCost and Orders
%     \end{solution}
% 
%   \begin{exercise} % 16
% 
% STEP 3: Using the information from exercise 14, Table \ref{tab:t12_16b} determine the coefficient of determination.
% 
% 
%     \framebox[7cm][l]{ Answer: }
%     \end{exercise}
%     % \begin{solution}  % 16
%     %
%     %    Read the results from the computer generated output.
%     % \end{solution}
% 
%   \begin{exercise} % 17
% 
%  STEP 4: Using the information from exercise nine, record the independent, dependent variables, and the correlation coefficients.
% 
% 
%   %  \framebox[7cm][l]{ Answer: }
%     \end{exercise}
%     \begin{solution}  % 17
% 
%       The results are in the table.
%     \end{solution}
% 
%   \begin{exercise} % 18
% 
% STEP 5:  Using the information from exercise nine, record the independent, dependent variables, and the correlation coefficients.
% 
%   %  \framebox[7cm][l]{ Answer: }
%   \end{exercise}
%   % \begin{solution}  % 18
%   %
%   % The results are in the table.
%   %   \end{solution}
% 
% \end{exercises}
% \onecolumn
% 
% 
% % <<label=lbl12_5a, results='asis', echo=FALSE>>=
% %   print(xtable(m12, caption='Coefficients', label="tab:12-7"), caption.placement="top")
% %
% % @
% 
% \begin{table}[ht]
%     \centering
%     \caption{exercise \# 7}
%     \begin{tabular}{lr} \hline
%         &  \multicolumn{1}{c}{Independent} \\ \hline
% 
%     Dep. Var. & 1. \underline{\phantom{xxxxxxxxxxxx}}      \\ \hline
%     1. \underline{\phantom{xxxxxxxxxxxx}}  &  \underline{\phantom{xxxxxxxxxxxx}}       \\ \hline
% 
%     \end{tabular}
%     \label{tab:t12_7}
% \end{table}
% 
% <<label=lbl12_15a, results='asis', echo=FALSE>>=
%   print(xtable(summary(m13), caption='Regression Analysis: Happiness vs. Age, Income, and Marital Status', label="tbl12_15a"), caption.placement="top")
% @
% 
%      \begin{table}[ht]
%      \centering
%      \caption{}
%      \begin{tabular}{lrrrr} \hline
%          &  \multicolumn{3}{c}{Independent} \\ \hline
% 
%      Dep. Var. & 1. \underline{\phantom{xxxxxxxxxx}} &
%                  2. \underline{\phantom{xxxxxxxxxx}} &
%                  3. \underline{\phantom{xxxxxxxxxx}} &
%                  4. \underline{\phantom{xxxxxxxxxx}} \\ \hline
%      1. \underline{\phantom{xxxxxxxxxxxx}}  &
%      \underline{\phantom{xxxxxxxxxxxx}}  \\ \hline
%      2. \underline{\phantom{xxxxxxxxxxxx}}  &
%      \underline{\phantom{xxxxxxxxxxxx}} &
%      \underline{\phantom{xxxxxxxxxxxx}}  \\ \hline
%      3. \underline{\phantom{xxxxxxxxxxxx}}  &
%      \underline{\phantom{xxxxxxxxxxxx}} &
%      \underline{\phantom{xxxxxxxxxxxx}} &
%      \underline{\phantom{xxxxxxxxxxxx}}  \\ \hline
%      4. \underline{\phantom{xxxxxxxxxxxx}}  &
%      \underline{\phantom{xxxxxxxxxxxx}} &
%      \underline{\phantom{xxxxxxxxxxxx}} &
%      \underline{\phantom{xxxxxxxxxxxx}} &
%      \underline{\phantom{xxxxxxxxxxxx}}  \\ \hline
%      \end{tabular}
%      \label{tab:t12_12}
%      \end{table}
% 
% <<label=lbl12-16, results="asis", echo=FALSE>>=
%     print(xtable(summary(m14), caption="Regression Analysis", label="tab:t12-16"), caption.placement="top")
% @
% 
%      \begin{table}[ht]
%      \centering
%      \caption{Correlation Coefficients}
%      \begin{tabular}{lrrrr} \hline
%          &  \multicolumn{3}{c}{Independent} \\ \hline
% 
%      Dep. Var. & 1. \underline{\phantom{xxxxxxxxxx}} &
%                  2. \underline{\phantom{xxxxxxxxxx}} &
%                  3. \underline{\phantom{xxxxxxxxxx}} &
%                  4. \underline{\phantom{xxxxxxxxxx}} \\ \hline
%      1. \underline{\phantom{xxxxxxxxxxxx}}  &
%      \underline{\phantom{xxxxxxxxxxxx}}  \\ \hline
%      2. \underline{\phantom{xxxxxxxxxxxx}}  &
%      \underline{\phantom{xxxxxxxxxxxx}} &
%      \underline{\phantom{xxxxxxxxxxxx}}  \\ \hline
%      3. \underline{\phantom{xxxxxxxxxxxx}}  &
%      \underline{\phantom{xxxxxxxxxxxx}} &
%      \underline{\phantom{xxxxxxxxxxxx}} &
%      \underline{\phantom{xxxxxxxxxxxx}}  \\ \hline
%      4. \underline{\phantom{xxxxxxxxxxxx}}  &
%      \underline{\phantom{xxxxxxxxxxxx}} &
%      \underline{\phantom{xxxxxxxxxxxx}} &
%      \underline{\phantom{xxxxxxxxxxxx}} &
%      \underline{\phantom{xxxxxxxxxxxx}}  \\ \hline
%      \end{tabular}
%      \label{tab:t12-16a}
%      \end{table}
% 
% <<label=lbl12_16b, results='asis', echo=FALSE>>=
%  print(xtable(aov16, caption="ANOVA", label="tab:t12_16b"), caption.placement="top")
% 
% @

